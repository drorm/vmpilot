import logging
from typing import Any

from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langgraph.prebuilt.chat_agent_executor import AgentState
from pydantic import SecretStr as PydanticSecretStr

from vmpilot.agent_logging import log_conversation_messages
from vmpilot.caching.chat_models import ChatAnthropic
from vmpilot.config import MAX_TOKENS, TEMPERATURE
from vmpilot.config import Provider as APIProvider
from vmpilot.config import config, current_provider
from vmpilot.prompt import get_system_prompt
from vmpilot.tools.setup_tools import setup_tools

logging.basicConfig(level=logging.INFO)
# Set logging levels for specific loggers
logger = logging.getLogger(__name__)

# Flag to enable beta features in Anthropic API
COMPUTER_USE_BETA_FLAG = "computer-use-2024-10-22"
PROMPT_CACHING_BETA_FLAG = "prompt-caching-2024-07-31"


"""
method to modify the state messages
Called by the agent to modify the messages generated by the agent
"""


def modify_state_messages(state: AgentState):
    # Keep the last N messages in the state as well as the system prompt

    # Handle system prompt with potential cache control
    messages = state["messages"]

    provider = current_provider.get()

    # Filter out messages with empty content first (for all providers)
    filtered_messages = []
    for message in messages:
        if not message.content:
            logger.warning(f"Filtering out message with empty content: {message}")
            continue
        filtered_messages.append(message)

    if provider is None or provider != APIProvider.ANTHROPIC:
        return filtered_messages

    # If the provider is Anthropic, add cache control to messages. https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching
    cached = 3  # antropic allows up to 4 and we use one for system
    for message in reversed(filtered_messages):
        if cached > 0:
            if isinstance(message.content, list):
                # Add cache control to the last block in the list (more efficient)
                for block in message.content:
                    if isinstance(block, dict):
                        block["cache_control"] = {"type": "ephemeral"}
                logger.debug(
                    f"Added cache_eph to list content blocks for message: {message}"
                )
            else:
                message.additional_kwargs["cache_control"] = {"type": "ephemeral"}
                logger.debug(
                    f"Added cache_eph message: {message}, additional_kwargs: {message.additional_kwargs}"
                )
            # Decrement cached once per message, not per block
            cached -= 1
        else:
            # Remove cache control from messages that shouldn't have it
            if isinstance(message.content, list):
                for block in message.content:
                    if isinstance(block, dict) and "cache_control" in block:
                        block.pop("cache_control", None)
            else:
                message.additional_kwargs.pop("cache_control", None)

    logger.debug(f"Modified state messages: {filtered_messages}")
    log_conversation_messages(list(filtered_messages), level="debug")
    return filtered_messages


"""
method to modify the state messages
Called by the agent to modify the messages generated by the agent
"""


"""
Create the langchain agent. The agent is in charge of the loop:
1. User request.
2. LLM response.
2. LLM invokation of one or more tools.
3. Sending the tool result to the LLM.
4. Rinse repeat until the llm decides it's done or we hit a recursion limit.
5. Detect when the llm is done.
"""


async def create_agent(
    model: str,
    api_key: str,
    provider: APIProvider,
    system_prompt_suffix: str = "",
    temperature: float = TEMPERATURE,
    max_tokens: int = MAX_TOKENS,
):
    """Create a LangChain agent with the configured tools."""
    enable_prompt_caching = False
    betas = [COMPUTER_USE_BETA_FLAG]

    # openai caches automatically
    if provider == APIProvider.ANTHROPIC:
        enable_prompt_caching = True

    if enable_prompt_caching:
        betas.append(PROMPT_CACHING_BETA_FLAG)

    if provider == APIProvider.ANTHROPIC:
        """We're jumping through hoops here to get the Anthropic caching to work correctly."""
        provider_config = config.get_provider_config(APIProvider.ANTHROPIC)
        if provider_config.beta_flags:
            betas.extend([flag for flag in provider_config.beta_flags.keys()])

        system_content: dict[str, Any] = {
            "type": "text",
            "text": get_system_prompt()
            + ("\n\n" + system_prompt_suffix if system_prompt_suffix else ""),
        }
        logger.debug(f"System prompt: {system_content}")

        if enable_prompt_caching:
            system_content["cache_control"] = {"type": "ephemeral"}

        llm = ChatAnthropic(
            model_name=model,
            temperature=temperature,
            stop=None,
            max_tokens_to_sample=max_tokens,
            api_key=PydanticSecretStr(api_key),
            timeout=30,  # Add 30-second timeout
            model_kwargs={
                "extra_headers": {"anthropic-beta": ",".join(betas)},
                "system": [system_content],
            },
        )
    elif provider == APIProvider.OPENAI:
        from langchain_openai import ChatOpenAI

        # Only set temperature=1 for o4-mini model
        provider_config = config.get_provider_config(APIProvider.OPENAI)
        model_temperature = 1 if model == "o4-mini" else temperature

        llm = ChatOpenAI(
            model=model,
            temperature=model_temperature,
            api_key=PydanticSecretStr(api_key),
            timeout=30,
        )
    elif provider == APIProvider.GOOGLE:
        # Create the Google AI LLM
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI

            llm = ChatGoogleGenerativeAI(
                model=model,
                temperature=temperature,
                timeout=30,
                google_api_key=api_key,  # type: ignore
            )
        except Exception as e:
            logging.error(f"Error creating Google AI LLM: {str(e)}")
            raise

    # Set up tools with LLM for fencing capability
    tools = setup_tools(llm=llm)

    # Create React agent
    agent = create_react_agent(
        llm,
        tools,
        state_modifier=modify_state_messages,  # type: ignore
        checkpointer=MemorySaver(),
    )

    return agent
